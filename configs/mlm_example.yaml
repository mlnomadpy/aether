# MLM Training Configuration Example
# Save this as mlm_config.yaml and use with: python train.py --config mlm_config.yaml

model:
  name: "minigpt-linear"
  architecture: "linear"
  maxlen: 512
  vocab_size: 50257
  embed_dim: 768
  num_heads: 12
  feed_forward_dim: 768
  num_transformer_blocks: 12
  dropout_rate: 0.1

training:
  # MLM Configuration
  training_mode: "mlm"  # "clm" for causal LM, "mlm" for masked LM
  mlm_mask_prob: 0.15   # Probability of masking each token
  mlm_replace_prob: 0.8 # Probability of replacing masked token with [MASK]
  mlm_random_prob: 0.1  # Probability of replacing masked token with random token
  final_evaluation: true # Run comprehensive evaluation at end of training
  
  # Standard Training Settings
  batch_size: 32
  learning_rate: 0.0001  # Lower LR often works better for MLM
  max_tokens_to_process: 1000000000
  eval_interval: 2000
  eval_steps: 1000
  val_set_size: 20000
  checkpoint_interval: 10000
  optimizer: "adamw"  # AdamW often works well for MLM
  weight_decay: 0.01
  
  # Learning Rate Scheduling
  lr_scheduler: "warmup_cosine"
  lr_scheduler_alpha: 0.1
  lr_scheduler_warmup_steps: 1000
  
  # Precision
  precision: "float32"

data:
  dataset_name: "HuggingFaceFW/fineweb"
  split: "train"
  streaming: true
  tokenizer_name: "gpt2"

logging:
  wandb_project: "aether-mlm-training"
  checkpoint_dir: "./checkpoints_mlm"
  log_level: "INFO"

device:
  auto_detect_mesh: true