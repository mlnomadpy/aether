# CLM Training Configuration Example (Traditional Causal Language Modeling)
# Save this as clm_config.yaml and use with: python train.py --config clm_config.yaml

model:
  name: "minigpt-linear"
  architecture: "linear"
  maxlen: 512
  vocab_size: 50257
  embed_dim: 768
  num_heads: 12
  feed_forward_dim: 768
  num_transformer_blocks: 12
  dropout_rate: 0.1

training:
  # CLM Configuration
  training_mode: "clm"  # "clm" for causal LM, "mlm" for masked LM
  final_evaluation: true # Run comprehensive evaluation at end of training
  
  # Standard Training Settings
  batch_size: 32
  learning_rate: 0.002  # Higher LR typically works for CLM
  max_tokens_to_process: 1000000000
  eval_interval: 2000
  eval_steps: 1000
  val_set_size: 20000
  checkpoint_interval: 10000
  optimizer: "novograd"  # Novograd often works well for CLM
  
  # Learning Rate Scheduling
  lr_scheduler: "constant"
  
  # Precision
  precision: "float32"

data:
  dataset_name: "HuggingFaceFW/fineweb"
  split: "train"
  streaming: true
  tokenizer_name: "gpt2"

logging:
  wandb_project: "aether-clm-training"
  checkpoint_dir: "./checkpoints_clm"
  log_level: "INFO"

device:
  auto_detect_mesh: true